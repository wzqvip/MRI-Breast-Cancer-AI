import requests
import json
import os
import zipfile
import time
import hashlib
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# ======================
#       CONFIG
# ======================

TCIA_FILE_PATH = "../QIN-Breast/QIN-BREAST_2015-09-04.tcia"
MAX_WORKERS = 13
DOWNLOAD_LOG_FILE = "download_failed.log"

# Whether to only download a subrange of the Series UIDs
USE_RANGE = False
START_INDEX = 22
END_INDEX = 30

# Number of retries if download fails
RETRY_LIMIT = 3
RETRY_SLEEP = 3  # seconds

# *NEW* Optional filter. By default, itâ€™s empty (i.e., download all).
# Example usage:
#   FILTER_DICT = {"Modality": "MR"}   # Only download Series where metadata has "Modality" == "MR"
#   FILTER_DICT = {"Modality": "CT", "Collection": "QIN-BREAST"}
#   FILTER_DICT = {}  # No filter (download everything)

FILTER_DICT = {}
# FILTER_DICT = {"Modality": "MR"}


# ======================
#   HELPER FUNCTIONS
# ======================

def parse_tcia_file(filepath: str) -> dict:
    """
    Parse a .tcia file to extract the configuration and the series UID list.
    """
    result = {}
    inside_series_list = False

    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            if "=" in line:
                if line.startswith("ListOfSeriesToDownload="):
                    inside_series_list = True
                    result["ListOfSeriesToDownload"] = []
                    _, value = line.split("=", 1)
                    if value.strip():
                        result["ListOfSeriesToDownload"].append(value.strip())
                else:
                    key, value = line.split("=", 1)
                    result[key.strip()] = value.strip()
            else:
                if inside_series_list:
                    result["ListOfSeriesToDownload"].append(line)
    return result

def get_series_metadata(series_uid: str) -> dict:
    """
    Retrieve metadata for the given SeriesInstanceUID using NBIA API.
    """
    base_url = 'https://services.cancerimagingarchive.net/nbia-api/services/v1/getSeriesMetaData'
    params = {'SeriesInstanceUID': series_uid}
    
    try:
        resp = requests.get(base_url, params=params)
        resp.raise_for_status()
        data = resp.json()
        if isinstance(data, list) and len(data) > 0:
            return data[0]
        return {}
    except Exception as e:
        print(f"[{series_uid}] ERROR: Failed to get metadata: {e}")
        return {}

def build_classification_path(metadata: dict) -> str:
    """
    Build a classification path based on metadata fields:
    Collection/SubjectID/StudyDate-StudyUIDLast5/SeriesDescFirstWord
    """
    collection = metadata.get("Collection", "UnknownCollection")
    subject_id = metadata.get("Subject ID", "UnknownSubjectID")
    study_uid  = metadata.get("Study UID", "UnknownStudyUID")
    study_date = metadata.get("Study Date", "UnknownDate")
    modality = metadata.get("Modality", "UnknownMethod")
    
    desc_full = metadata.get("Series Description", "NoDesc")
    desc_first = desc_full.split()[0] if desc_full else "NoDesc"
    
    uid_last_5 = study_uid[-5:] if len(study_uid) >= 5 else study_uid
    date_suffix = f"{study_date}-{uid_last_5}"
    
    path = os.path.join(collection, subject_id, date_suffix, modality, desc_first)
    return path

def compute_file_md5(filepath: str) -> str:
    """
    Compute the MD5 checksum of the file at filepath.
    """
    md5_hash = hashlib.md5()
    with open(filepath, 'rb') as f:
        for chunk in iter(lambda: f.read(8192), b''):
            md5_hash.update(chunk)
    return md5_hash.hexdigest()

def verify_md5_in_folder(folder_path: str) -> bool:
    """
    Look for an MD5 CSV file in the folder (generated by getImageWithMD5Hash).
    Compare each DICOM file's MD5 with the CSV reference.
    Returns True if all checks pass, otherwise False.
    """
    csv_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.csv')]
    if not csv_files:
        print(f"[{folder_path}] No MD5 CSV found. Skipping verification.")
        return True

    csv_path = os.path.join(folder_path, csv_files[0])

    with open(csv_path, 'r', encoding='utf-8') as f:
        lines = f.read().strip().splitlines()

    # Remove header if present
    if lines and 'SOPInstanceUID' in lines[0]:
        lines = lines[1:]

    for line in lines:
        cols = line.split(',')
        if len(cols) < 2:
            continue

        sop_uid, ref_md5 = cols[0].strip(), cols[1].strip()
        
        dicom_path = os.path.join(folder_path, f"{sop_uid}.dcm")
        if not os.path.exists(dicom_path):
            print(f"[{folder_path}] Missing file for SOP: {sop_uid}")
            return False
        
        actual_md5 = compute_file_md5(dicom_path)
        # Compare both values in lowercase
        if actual_md5.lower() != ref_md5.lower():
            print(f"[{folder_path}] MD5 mismatch: {dicom_path}")
            print(f"    Expected: {ref_md5.lower()}")
            print(f"    Actual:   {actual_md5.lower()}")
            return False

    print(f"[{folder_path}] All files match MD5 hashes.")
    return True


def download_series_with_md5(series_uid: str, out_zip_path: str) -> bool:
    """
    Download the series as a ZIP file using 'getImageWithMD5Hash', showing progress with tqdm.
    Returns True if successful, else False.
    """
    base_download_url = 'https://services.cancerimagingarchive.net/nbia-api/services/v1/getImageWithMD5Hash'
    download_url = f"{base_download_url}?SeriesInstanceUID={series_uid}"
    
    try:
        head_resp = requests.head(download_url)
        head_resp.raise_for_status()
        total_size = int(head_resp.headers.get('Content-Length', 0))
        
        with requests.get(download_url, stream=True) as r, open(out_zip_path, 'wb') as f:
            r.raise_for_status()
            progress_bar = tqdm(total=total_size, unit='B', unit_scale=True,
                                desc=f"Downloading {series_uid[-10:]}")
            chunk_size = 1024 * 256
            for chunk in r.iter_content(chunk_size=chunk_size):
                if chunk:
                    f.write(chunk)
                    progress_bar.update(len(chunk))
            progress_bar.close()
        return True
    except Exception as e:
        print(f"[{series_uid}] ERROR: Download failed: {e}")
        return False

def extract_zip_to_folder(zip_path: str, extract_dir: str) -> bool:
    """
    Extract the ZIP file to the target folder.
    Returns True if successful, else False.
    """
    try:
        os.makedirs(extract_dir, exist_ok=True)
        with zipfile.ZipFile(zip_path, 'r') as zf:
            zf.extractall(path=extract_dir)
        return True
    except zipfile.BadZipFile as e:
        print(f"[{zip_path}] ERROR: Bad ZIP file: {e}")
    except Exception as e:
        print(f"[{zip_path}] ERROR: Extraction failed: {e}")
    return False

def passes_filter(metadata: dict) -> bool:
    """
    Check if the given metadata satisfies the FILTER_DICT constraints.
    If FILTER_DICT is empty, we download everything.
    Otherwise, all specified keys must match exactly.
    """
    if not FILTER_DICT:
        return True  # No filter set -> always pass

    for k, v in FILTER_DICT.items():
        if str(metadata.get(k, "")) != str(v):
            return False
    return True

def attempt_download_and_extract(series_uid: str) -> bool:
    """
    Attempt the full process up to RETRY_LIMIT times:
    1) Get metadata
    2) Check if metadata passes the user-specified filter
    3) Build folder path
    4) Download zip with MD5
    5) Extract zip
    6) Verify MD5 checks
    Returns True if succeeded, otherwise False after all retries.
    """
    metadata = get_series_metadata(series_uid)
    if not metadata:
        return False
    
    # Check filter
    if not passes_filter(metadata):
        print(f"[{series_uid}] Skipped due to filter constraints.")
        return True  # Return True so it doesn't get logged as a failure.

    classification_path = build_classification_path(metadata)
    os.makedirs(classification_path, exist_ok=True)
    
    zip_name = series_uid.replace('.', '_') + ".zip"
    zip_path = os.path.join(classification_path, zip_name)
    
    for attempt in range(RETRY_LIMIT):
        print(f"[{series_uid}] Attempt {attempt+1}/{RETRY_LIMIT} -> {zip_path}")
        success_download = download_series_with_md5(series_uid, zip_path)
        if not success_download:
            print(f"[{series_uid}] Download failed. Will retry in {RETRY_SLEEP}s.")
            time.sleep(RETRY_SLEEP)
            continue
        
        print(f"[{series_uid}] Extracting -> {classification_path}")
        if not extract_zip_to_folder(zip_path, classification_path):
            print(f"[{series_uid}] Extraction failed. Will retry in {RETRY_SLEEP}s.")
            time.sleep(RETRY_SLEEP)
            continue
        
        if verify_md5_in_folder(classification_path):
            print(f"[{series_uid}] MD5 verification succeeded.")
            # Remove ZIP if no longer needed
            try:
                os.remove(zip_path)
            except OSError:
                pass
            return True
        else:
            print(f"[{series_uid}] MD5 mismatch. Will retry in {RETRY_SLEEP}s.")
            time.sleep(RETRY_SLEEP)
    
    return False

def main():
    parsed_data = parse_tcia_file(TCIA_FILE_PATH)
    series_uids = parsed_data.get("ListOfSeriesToDownload", [])
    if not series_uids:
        print("No Series UIDs found in the .tcia file.")
        return
    
    # Optional: only download a subrange
    if USE_RANGE:
        series_uids = series_uids[START_INDEX:END_INDEX]

    print(f"Found {len(series_uids)} Series UIDs, starting {MAX_WORKERS} thread(s) download...")

    # Prepare a log file for permanent failures
    if os.path.exists(DOWNLOAD_LOG_FILE):
        os.remove(DOWNLOAD_LOG_FILE)  # Clear old log
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_map = {executor.submit(attempt_download_and_extract, uid): uid for uid in series_uids}
        for future in as_completed(future_map):
            uid = future_map[future]
            try:
                success = future.result()
                if not success:
                    with open(DOWNLOAD_LOG_FILE, 'a', encoding='utf-8') as f:
                        f.write(uid + "\n")
                    print(f"[{uid}] ERROR: Failed after {RETRY_LIMIT} attempts. Logged.")
            except Exception as e:
                print(f"[{uid}] ERROR: Unexpected exception: {e}")
                with open(DOWNLOAD_LOG_FILE, 'a', encoding='utf-8') as f:
                    f.write(f"{uid} - Exception: {str(e)}\n")

if __name__ == "__main__":
    main()
